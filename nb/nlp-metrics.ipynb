{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 1.0000\n",
      "ROUGE-2: 0.5000\n",
      "ROUGE-L: 0.7143\n",
      "BLEU: 0.3826\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def rouge_n(reference, candidate, n):\n",
    "    ref_ngrams = set(ngrams(word_tokenize(reference.lower()), n))\n",
    "    cand_ngrams = set(ngrams(word_tokenize(candidate.lower()), n))\n",
    "    \n",
    "    overlap = len(ref_ngrams.intersection(cand_ngrams))\n",
    "    total = len(ref_ngrams)\n",
    "    \n",
    "    return overlap / total if total > 0 else 0\n",
    "\n",
    "def rouge_l(reference, candidate):\n",
    "    ref_tokens = word_tokenize(reference.lower())\n",
    "    cand_tokens = word_tokenize(candidate.lower())\n",
    "    \n",
    "    m, n = len(ref_tokens), len(cand_tokens)\n",
    "    lcs_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if ref_tokens[i-1] == cand_tokens[j-1]:\n",
    "                lcs_table[i][j] = lcs_table[i-1][j-1] + 1\n",
    "            else:\n",
    "                lcs_table[i][j] = max(lcs_table[i-1][j], lcs_table[i][j-1])\n",
    "    \n",
    "    lcs_length = lcs_table[m][n]\n",
    "    return lcs_length / len(ref_tokens)\n",
    "\n",
    "def bleu_score(reference, candidate, max_n=4):\n",
    "    ref_tokens = word_tokenize(reference.lower())\n",
    "    cand_tokens = word_tokenize(candidate.lower())\n",
    "    \n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter(ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(ngrams(cand_tokens, n))\n",
    "        \n",
    "        total_ngrams = sum(cand_ngrams.values())\n",
    "        matching_ngrams = sum((ref_ngrams & cand_ngrams).values())\n",
    "        \n",
    "        precisions.append(matching_ngrams / total_ngrams if total_ngrams > 0 else 0)\n",
    "    \n",
    "    bp = min(1, math.exp(1 - len(ref_tokens) / len(cand_tokens))) if len(cand_tokens) > 0 else 0\n",
    "    \n",
    "    return bp * math.exp(sum(math.log(p) for p in precisions if p > 0) / max_n)\n",
    "\n",
    "def compare_metrics(reference, candidate):\n",
    "    rouge_1 = rouge_n(reference, candidate, 1)\n",
    "    rouge_2 = rouge_n(reference, candidate, 2)\n",
    "    rouge_l_score = rouge_l(reference, candidate)\n",
    "    bleu = bleu_score(reference, candidate)\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_1:.4f}\")\n",
    "    print(f\"ROUGE-2: {rouge_2:.4f}\")\n",
    "    print(f\"ROUGE-L: {rouge_l_score:.4f}\")\n",
    "    print(f\"BLEU: {bleu:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "reference = \"The cat is on the mat.\"\n",
    "candidate = \"There is a cat on the mat.\"\n",
    "\n",
    "compare_metrics(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore - Precision: 0.6806, Recall: 0.7502, F1: 0.7137\n",
      "BERTScore - Precision: 0.7620, Recall: 0.7814, F1: 0.7716\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.squeeze(0).numpy()\n",
    "\n",
    "def bertscore_f1(reference, candidate):\n",
    "    # Get BERT embeddings\n",
    "    ref_embeddings = get_bert_embeddings(reference)\n",
    "    cand_embeddings = get_bert_embeddings(candidate)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(ref_embeddings, cand_embeddings)\n",
    "    \n",
    "    # Compute precision and recall\n",
    "    precision = similarity_matrix.max(axis=0).mean()\n",
    "    recall = similarity_matrix.max(axis=1).mean()\n",
    "    \n",
    "    # Compute F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Example usage\n",
    "reference = \"The cat is sitting on the mat.\"\n",
    "candidate = \"A feline is resting on the floor covering.\"\n",
    "\n",
    "scores = bertscore_f1(reference, candidate)\n",
    "print(f\"BERTScore - Precision: {scores['precision']:.4f}, Recall: {scores['recall']:.4f}, F1: {scores['f1']:.4f}\")\n",
    "\n",
    "# Compare with a less semantically similar candidate\n",
    "candidate2 = \"The dog is running in the park.\"\n",
    "scores2 = bertscore_f1(reference, candidate2)\n",
    "print(f\"BERTScore - Precision: {scores2['precision']:.4f}, Recall: {scores2['recall']:.4f}, F1: {scores2['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches (reference_index, candidate_index, similarity_score):\n",
      "Reference word 0 matched with candidate word 2, score: 1.0000\n",
      "Reference word 1 matched with candidate word 1, score: 0.9999\n",
      "Reference word 2 matched with candidate word 0, score: 0.9988\n",
      "\n",
      "Unmatched reference words: set()\n",
      "Unmatched candidate words: set()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def greedy_matching(reference_embeddings, candidate_embeddings, threshold=0):\n",
    "    matches = []\n",
    "    unmatched_candidate_indices = list(range(len(candidate_embeddings)))\n",
    "    \n",
    "    for i, ref_embedding in enumerate(reference_embeddings):\n",
    "        best_match = -1\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for j in unmatched_candidate_indices:\n",
    "            cand_embedding = candidate_embeddings[j]\n",
    "            score = cosine_similarity(ref_embedding, cand_embedding)\n",
    "            \n",
    "            if score > best_score and score > threshold:\n",
    "                best_match = j\n",
    "                best_score = score\n",
    "        \n",
    "        if best_match != -1:\n",
    "            matches.append((i, best_match, best_score))\n",
    "            unmatched_candidate_indices.remove(best_match)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example usage\n",
    "reference = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])  # 3 reference \"words\"\n",
    "candidate = np.array([[1, 1, 0.9], [3, 3, 3.1], [4, 4, 4]])  # 3 candidate \"words\"\n",
    "\n",
    "matches = greedy_matching(reference, candidate, threshold=0.9)\n",
    "\n",
    "print(\"Matches (reference_index, candidate_index, similarity_score):\")\n",
    "for match in matches:\n",
    "    print(f\"Reference word {match[0]} matched with candidate word {match[1]}, score: {match[2]:.4f}\")\n",
    "\n",
    "print(\"\\nUnmatched reference words:\", set(range(len(reference))) - set(m[0] for m in matches))\n",
    "print(\"Unmatched candidate words:\", set(range(len(candidate))) - set(m[1] for m in matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
