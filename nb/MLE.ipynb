{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesGenerative:\n",
    "    def fit(self, X, y):\n",
    "        # Estimate P(y) - class probabilities\n",
    "        self.class_probs = np.bincount(y) / len(y)\n",
    "        \n",
    "        # Estimate P(x|y) - feature distributions per class\n",
    "        self.feature_probs = {}\n",
    "        for class_label in [0, 1]:\n",
    "            class_data = X[y == class_label]\n",
    "            # Estimate feature distributions for this class\n",
    "            self.feature_probs[class_label] = self._estimate_distributions(class_data)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Using Bayes rule: P(y|x) ∝ P(x|y)P(y)\n",
    "        return self._compute_posterior(X)\n",
    "\n",
    "class LogisticDiscriminative:\n",
    "    def fit(self, X, y):\n",
    "        # Directly model P(y|x) without modeling P(x|y)\n",
    "        self.weights = self._optimize_likelihood(X, y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Directly estimate P(y|x)\n",
    "        return self._sigmoid(X @ self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.36103492, -0.49929923],\n",
       "        [-1.32242966,  0.2006002 ],\n",
       "        [ 0.31985125,  0.08571429],\n",
       "        ...,\n",
       "        [ 2.40526228,  3.1081587 ],\n",
       "        [ 1.40294171,  1.34989919],\n",
       "        [ 1.63634125,  1.31020732]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([[ 0.4138122 , -0.08701867],\n",
       "        [ 1.41868389,  1.74139114],\n",
       "        [-1.21134793, -1.23180853],\n",
       "        [ 2.85625515, -0.0289158 ],\n",
       "        [-0.41438399, -1.03879557],\n",
       "        [ 1.0941371 ,  2.82004484],\n",
       "        [ 1.305831  ,  1.2715837 ],\n",
       "        [ 2.15302515,  2.37094074],\n",
       "        [ 0.50781368,  0.90935818],\n",
       "        [ 4.86241354, -0.79616056]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GenerativeClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_means = {}\n",
    "        self.class_covs = {}\n",
    "        self.class_priors = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # For each class, we model P(x|y) as a Gaussian\n",
    "        # and P(y) as the empirical class frequency\n",
    "        unique_classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            # Get samples for this class\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Estimate P(y) - class prior\n",
    "            self.class_priors[c] = len(X_c) / n_samples\n",
    "            \n",
    "            # Estimate P(x|y) - class-conditional density\n",
    "            self.class_means[c] = np.mean(X_c, axis=0)\n",
    "            self.class_covs[c] = np.cov(X_c, rowvar=False)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Using Bayes rule: P(y|x) ∝ P(x|y)P(y)\n",
    "        probs = []\n",
    "        for c in self.class_priors:\n",
    "            # Calculate P(x|y)\n",
    "            likelihood = multivariate_normal.pdf(\n",
    "                X, \n",
    "                mean=self.class_means[c], \n",
    "                cov=self.class_covs[c]\n",
    "            )\n",
    "            # Multiply by P(y)\n",
    "            class_prob = likelihood * self.class_priors[c]\n",
    "            probs.append(class_prob)\n",
    "            \n",
    "        # Normalize to get probabilities\n",
    "        probs = np.array(probs).T\n",
    "        return probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "class DiscriminativeClassifier:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y, epochs=100):\n",
    "        # Directly model P(y|x) through logistic regression\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # Forward pass\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Gradient descent\n",
    "            dw = X.T.dot(y_pred - y) / len(y)\n",
    "            db = np.mean(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "def demonstrate_model_differences():\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Class 0: Gaussian centered at (0,0)\n",
    "    X0 = np.random.multivariate_normal([0,0], [[1,0.5],[0.5,1]], n_samples//2)\n",
    "    # Class 1: Gaussian centered at (2,2)\n",
    "    X1 = np.random.multivariate_normal([2,2], [[1.5,-0.5],[-0.5,1.5]], n_samples//2)\n",
    "    \n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0]*(n_samples//2) + [1]*(n_samples//2))\n",
    "    \n",
    "    # Train both models\n",
    "    gen_model = GenerativeClassifier()\n",
    "    disc_model = DiscriminativeClassifier()\n",
    "    \n",
    "    gen_model.fit(X, y)\n",
    "    disc_model.fit(X, y)\n",
    "    \n",
    "    # Now we can show a key difference:\n",
    "    # Generative model can generate new samples\n",
    "    def generate_samples(gen_model, n_samples=10):\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            # First sample a class according to P(y)\n",
    "            class_idx = np.random.choice(\n",
    "                list(gen_model.class_priors.keys()),\n",
    "                p=list(gen_model.class_priors.values())\n",
    "            )\n",
    "            # Then sample from P(x|y) for that class\n",
    "            sample = np.random.multivariate_normal(\n",
    "                gen_model.class_means[class_idx],\n",
    "                gen_model.class_covs[class_idx]\n",
    "            )\n",
    "            samples.append(sample)\n",
    "        return np.array(samples)\n",
    "    \n",
    "    # Generate new samples\n",
    "    new_samples = generate_samples(gen_model)\n",
    "    # Discriminative model cannot generate samples!\n",
    "    \n",
    "    return X, y, new_samples\n",
    "\n",
    "demonstrate_model_differences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio_test(complex_model, simple_model, data):\n",
    "    # Compute log-likelihoods\n",
    "    ll_complex = complex_model.log_likelihood(data)\n",
    "    ll_simple = simple_model.log_likelihood(data)\n",
    "    \n",
    "    # Compute test statistic\n",
    "    lr_statistic = 2 * (ll_complex - ll_simple)\n",
    "    \n",
    "    # Degrees of freedom = difference in number of parameters\n",
    "    df = complex_model.n_params - simple_model.n_params\n",
    "    \n",
    "    # p-value from chi-squared distribution\n",
    "    from scipy.stats import chi2\n",
    "    p_value = 1 - chi2.cdf(lr_statistic, df)\n",
    "    \n",
    "    return lr_statistic, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
