{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5: 1.000\n",
      "NDCG@10: 0.999\n",
      "NDCG (full): 0.997\n",
      "\n",
      "Example rankings for new documents:\n",
      "   Document_ID  Rank  Predicted_Score\n",
      "4            4     1         0.657692\n",
      "1            1     2         0.438163\n",
      "2            2     3         0.396746\n",
      "3            3     4         0.350979\n",
      "0            0     5        -0.229912\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "import pandas as pd\n",
    "\n",
    "class PointwiseRankingModel:\n",
    "    def __init__(self):\n",
    "        self.model = LinearRegression()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_data(self, features):\n",
    "        \"\"\"\n",
    "        Prepare features by scaling them to have zero mean and unit variance\n",
    "        \n",
    "        Parameters:\n",
    "        - features: Document features matrix (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - Scaled features matrix\n",
    "        \"\"\"\n",
    "        return self.scaler.fit_transform(features)\n",
    "    \n",
    "    def fit(self, features, relevance_scores):\n",
    "        \"\"\"\n",
    "        Train the ranking model\n",
    "        \n",
    "        Parameters:\n",
    "        - features: Document features matrix (n_samples, n_features)\n",
    "        - relevance_scores: Target relevance scores (n_samples,)\n",
    "        \"\"\"\n",
    "        X_scaled = self.prepare_data(features)\n",
    "        self.model.fit(X_scaled, relevance_scores)\n",
    "        \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predict relevance scores for new documents\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(features)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def rank_documents(self, features, return_scores=False):\n",
    "        \"\"\"\n",
    "        Rank documents based on their predicted relevance scores\n",
    "        \n",
    "        Parameters:\n",
    "        - features: Document features matrix\n",
    "        - return_scores: If True, return both rankings and scores\n",
    "        \n",
    "        Returns:\n",
    "        - Document rankings (and scores if return_scores=True)\n",
    "        \"\"\"\n",
    "        scores = self.predict(features)\n",
    "        rankings = np.argsort(-scores)  # Sort in descending order\n",
    "        \n",
    "        if return_scores:\n",
    "            return rankings, scores\n",
    "        return rankings\n",
    "\n",
    "def evaluate_ranking(y_true, y_pred, k=None):\n",
    "    \"\"\"\n",
    "    Evaluate ranking performance using NDCG\n",
    "    \"\"\"\n",
    "    # Reshape relevance scores for sklearn's ndcg_score\n",
    "    y_true = np.array([y_true])\n",
    "    y_pred = np.array([y_pred])\n",
    "    \n",
    "    if k is None:\n",
    "        return ndcg_score(y_true, y_pred)\n",
    "    return ndcg_score(y_true, y_pred, k=k)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_features = 5\n",
    "    \n",
    "    # Create synthetic features (e.g., text length, freshness, popularity, etc.)\n",
    "    features = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate synthetic relevance scores (0 to 4, where 4 is most relevant)\n",
    "    # We'll create these using a linear combination of features plus noise\n",
    "    true_weights = np.array([0.6, 0.3, 0.2, 0.1, -0.1])\n",
    "    relevance_scores = np.dot(features, true_weights)\n",
    "    relevance_scores += np.random.normal(0, 0.1, n_samples)  # Add noise\n",
    "    relevance_scores = np.clip(relevance_scores, 0, 4)  # Clip to valid range\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, relevance_scores, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    ranker = PointwiseRankingModel()\n",
    "    ranker.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = ranker.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    ndcg_5 = evaluate_ranking(y_test, y_pred, k=5)\n",
    "    ndcg_10 = evaluate_ranking(y_test, y_pred, k=10)\n",
    "    ndcg_full = evaluate_ranking(y_test, y_pred)\n",
    "    \n",
    "    print(f\"NDCG@5: {ndcg_5:.3f}\")\n",
    "    print(f\"NDCG@10: {ndcg_10:.3f}\")\n",
    "    print(f\"NDCG (full): {ndcg_full:.3f}\")\n",
    "    \n",
    "    # Example: Rank new documents\n",
    "    new_documents = np.random.randn(5, n_features)  # 5 new documents\n",
    "    rankings, scores = ranker.rank_documents(new_documents, return_scores=True)\n",
    "    \n",
    "    print(\"\\nExample rankings for new documents:\")\n",
    "    results = pd.DataFrame({\n",
    "        'Document_ID': range(len(rankings)),\n",
    "        'Rank': np.argsort(rankings) + 1,\n",
    "        'Predicted_Score': scores\n",
    "    })\n",
    "    print(results.sort_values('Rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching documents for queries:\n",
      "\n",
      "Query: 'quick brown fox'\n",
      "Document 0: 'The quick brown fox jumps over the lazy dog' (Score: 2.276)\n",
      "Document 1: 'Quick brown foxes jump over lazy dogs' (Score: 1.118)\n",
      "Document 3: 'A quick brown dog runs in the park' (Score: 1.053)\n",
      "\n",
      "Query: 'lazy dog sleeps'\n",
      "Document 2: 'The lazy dog sleeps in the sun' (Score: 2.555)\n",
      "Document 0: 'The quick brown fox jumps over the lazy dog' (Score: 0.995)\n",
      "Document 1: 'Quick brown foxes jump over lazy dogs' (Score: 0.559)\n",
      "\n",
      "Query: 'foxes and dogs'\n",
      "Document 4: 'Foxes and dogs play in the garden' (Score: 3.253)\n",
      "Document 1: 'Quick brown foxes jump over lazy dogs' (Score: 1.815)\n",
      "Document 0: 'The quick brown fox jumps over the lazy dog' (Score: 0.000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        \"\"\"\n",
    "        Initialize BM25 with hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            k1: Term frequency saturation parameter (default: 1.5)\n",
    "            b: Length normalization parameter (default: 0.75)\n",
    "        \"\"\"\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        # Initialize required variables\n",
    "        self.doc_freqs = defaultdict(int)  # df values for each term\n",
    "        self.idf = defaultdict(float)      # idf values for each term\n",
    "        self.doc_lens = []                 # length of each document\n",
    "        self.avgdl = 0                     # average document length\n",
    "        self.total_docs = 0                # total number of documents\n",
    "        self.doc_terms = []                # terms in each document\n",
    "        \n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Preprocess text by converting to lowercase and splitting into terms.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "        \n",
    "        Returns:\n",
    "            List of preprocessed terms\n",
    "        \"\"\"\n",
    "        # Convert to lowercase and split into terms\n",
    "        text = text.lower()\n",
    "        # Remove special characters and split into terms\n",
    "        terms = re.findall(r'\\w+', text)\n",
    "        return terms\n",
    "    \n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Fit BM25 parameters on a collection of documents.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document strings\n",
    "        \"\"\"\n",
    "        self.total_docs = len(documents)\n",
    "        \n",
    "        # Process each document\n",
    "        for document in documents:\n",
    "            terms = self.preprocess(document)\n",
    "            self.doc_lens.append(len(terms))\n",
    "            \n",
    "            # Count term frequencies in document\n",
    "            term_freqs = Counter(terms)\n",
    "            self.doc_terms.append(term_freqs)\n",
    "            \n",
    "            # Update document frequencies\n",
    "            for term in set(terms):\n",
    "                self.doc_freqs[term] += 1\n",
    "        \n",
    "        # Calculate average document length\n",
    "        self.avgdl = sum(self.doc_lens) / self.total_docs\n",
    "        \n",
    "        # Calculate IDF for each term\n",
    "        for term, df in self.doc_freqs.items():\n",
    "            idf = math.log((self.total_docs - df + 0.5) / (df + 0.5) + 1.0)\n",
    "            self.idf[term] = idf\n",
    "    \n",
    "    def get_score(self, query: str, doc_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate BM25 score for a query and document.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            doc_idx: Index of the document\n",
    "        \n",
    "        Returns:\n",
    "            BM25 score\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        query_terms = self.preprocess(query)\n",
    "        doc_terms = self.doc_terms[doc_idx]\n",
    "        doc_len = self.doc_lens[doc_idx]\n",
    "        \n",
    "        # Calculate score for each query term\n",
    "        for term in query_terms:\n",
    "            if term not in self.idf:\n",
    "                continue\n",
    "                \n",
    "            # Get term frequency in document\n",
    "            tf = doc_terms.get(term, 0)\n",
    "            \n",
    "            # Calculate normalized term frequency\n",
    "            numerator = tf * (self.k1 + 1)\n",
    "            denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            \n",
    "            # Add contribution of current term to score\n",
    "            score += self.idf[term] * (numerator / denominator)\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def get_top_n(self, query: str, documents: List[str], n: int = 5) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Get top N documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query string\n",
    "            documents: List of documents\n",
    "            n: Number of top documents to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (document_index, score) tuples\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for idx in range(self.total_docs):\n",
    "            score = self.get_score(query, idx)\n",
    "            scores.append((idx, score))\n",
    "        \n",
    "        # Sort by score in descending order\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Quick brown foxes jump over lazy dogs\",\n",
    "        \"The lazy dog sleeps in the sun\",\n",
    "        \"A quick brown dog runs in the park\",\n",
    "        \"Foxes and dogs play in the garden\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize and fit BM25\n",
    "    bm25 = BM25(k1=1.5, b=0.75)\n",
    "    bm25.fit(documents)\n",
    "    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"quick brown fox\",\n",
    "        \"lazy dog sleeps\",\n",
    "        \"foxes and dogs\"\n",
    "    ]\n",
    "    \n",
    "    # Get top results for each query\n",
    "    print(\"Top matching documents for queries:\\n\")\n",
    "    for query in queries:\n",
    "        print(f\"Query: '{query}'\")\n",
    "        top_docs = bm25.get_top_n(query, documents, n=3)\n",
    "        for doc_id, score in top_docs:\n",
    "            print(f\"Document {doc_id}: '{documents[doc_id]}' (Score: {score:.3f})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "Document 0: 'The quick brown fox jumps over the lazy dog' (Score: 0.596)\n",
      "Document 1: 'Quick brown foxes jump over lazy dogs' (Score: 0.258)\n",
      "Document 3: 'A quick brown dog runs in the park' (Score: 0.226)\n",
      "\n",
      "Document Similarities:\n",
      "Similarity between doc 0 and 1: 0.356\n",
      "Similarity between doc 1 and 2: 0.085\n",
      "Similarity between doc 2 and 3: 0.249\n",
      "Similarity between doc 3 and 4: 0.114\n",
      "\n",
      "Document Vectors (first 5 terms):\n",
      "Doc 0: {'the': 0.4263810613259306, 'brown': 0.2578563540460651, 'lazy': 0.2578563540460651, 'dog': 0.2578563540460651, 'quick': 0.2578563540460651}\n",
      "Doc 1: {'the': 0.0, 'brown': 0.29839156796488747, 'lazy': 0.29839156796488747, 'dog': 0.0, 'quick': 0.29839156796488747}\n",
      "Doc 2: {'the': 0.46941847923824787, 'brown': 0.0, 'lazy': 0.28388347550384463, 'dog': 0.28388347550384463, 'quick': 0.0}\n",
      "Doc 3: {'the': 0.21567813286992524, 'brown': 0.26086513700377395, 'lazy': 0.0, 'dog': 0.26086513700377395, 'quick': 0.26086513700377395}\n",
      "Doc 4: {'the': 0.21377556692631727, 'brown': 0.0, 'lazy': 0.0, 'dog': 0.0, 'quick': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def __init__(self, use_idf=True, norm=\"l2\"):\n",
    "        \"\"\"\n",
    "        Initialize TF-IDF Vectorizer\n",
    "\n",
    "        Args:\n",
    "            use_idf: Whether to use IDF weighting\n",
    "            norm: Normalization method ('l1', 'l2', or None)\n",
    "        \"\"\"\n",
    "        self.vocabulary = {}\n",
    "        self.doc_freq = None\n",
    "        self.use_idf = use_idf\n",
    "        self.norm = norm\n",
    "        self.n_docs = 0\n",
    "        self.idf = None\n",
    "\n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        \"\"\"Preprocess text into tokens\"\"\"\n",
    "        # Convert to lowercase and split into terms\n",
    "        text = text.lower()\n",
    "        terms = re.findall(r\"\\w+\", text)\n",
    "        return terms\n",
    "\n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Fit vectorizer to a collection of documents\n",
    "\n",
    "        Args:\n",
    "            documents: List of document strings\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        term_doc_freq = Counter()\n",
    "        for doc in documents:\n",
    "            terms = self.preprocess(doc)\n",
    "            # Add unique terms from this document\n",
    "            term_doc_freq.update(set(terms))\n",
    "\n",
    "        # Create vocabulary with term indices\n",
    "        self.vocabulary = {\n",
    "            term: idx for idx, (term, _) in enumerate(term_doc_freq.most_common())\n",
    "        }\n",
    "\n",
    "        # Calculate IDF\n",
    "        self.n_docs = len(documents)\n",
    "        if self.use_idf:\n",
    "            self.idf = np.zeros(len(self.vocabulary))\n",
    "            for term, idx in self.vocabulary.items():\n",
    "                self.idf[idx] = np.log(self.n_docs / term_doc_freq[term] + 1)\n",
    "\n",
    "    def transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform documents to TF-IDF matrix\n",
    "\n",
    "        Args:\n",
    "            documents: List of document strings\n",
    "\n",
    "        Returns:\n",
    "            Document-term matrix with TF-IDF weights\n",
    "        \"\"\"\n",
    "        # Initialize document-term matrix\n",
    "        X = np.zeros((len(documents), len(self.vocabulary)))\n",
    "\n",
    "        # Fill in TF values\n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            terms = self.preprocess(doc)\n",
    "            term_freq = Counter(terms)\n",
    "\n",
    "            for term, freq in term_freq.items():\n",
    "                if term in self.vocabulary:\n",
    "                    term_idx = self.vocabulary[term]\n",
    "                    X[doc_idx, term_idx] = freq\n",
    "\n",
    "        # Apply IDF weights\n",
    "        if self.use_idf:\n",
    "            X = X * self.idf\n",
    "\n",
    "        # Apply normalization\n",
    "        if self.norm:\n",
    "            X = normalize(X, norm=self.norm, axis=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"Convenience method to fit and transform in one step\"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n",
    "\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Return list of terms in vocabulary\"\"\"\n",
    "        sorted_vocab = sorted(self.vocabulary.items(), key=lambda x: x[1])\n",
    "        return [term for term, _ in sorted_vocab]\n",
    "\n",
    "\n",
    "class VectorSpaceModel:\n",
    "    def __init__(self, use_idf=True, norm=\"l2\"):\n",
    "        \"\"\"\n",
    "        Initialize Vector Space Model\n",
    "\n",
    "        Args:\n",
    "            use_idf: Whether to use IDF weighting\n",
    "            norm: Normalization method ('l1', 'l2', or None)\n",
    "        \"\"\"\n",
    "        self.vectorizer = TFIDFVectorizer(use_idf=use_idf, norm=norm)\n",
    "        self.document_vectors = None\n",
    "        self.documents = None\n",
    "\n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Fit model to documents\n",
    "\n",
    "        Args:\n",
    "            documents: List of document strings\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.document_vectors = self.vectorizer.fit_transform(documents)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Search for documents similar to query\n",
    "\n",
    "        Args:\n",
    "            query: Query string\n",
    "            top_k: Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List of (document_index, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Transform query to vector\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for doc_idx in range(len(self.documents)):\n",
    "            sim = 1 - cosine(query_vector[0], self.document_vectors[doc_idx])\n",
    "            similarities.append((doc_idx, sim))\n",
    "\n",
    "        # Sort by similarity and return top k\n",
    "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def document_similarity(self, doc_id1: int, doc_id2: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate similarity between two documents\n",
    "\n",
    "        Args:\n",
    "            doc_id1: Index of first document\n",
    "            doc_id2: Index of second document\n",
    "\n",
    "        Returns:\n",
    "            Cosine similarity between documents\n",
    "        \"\"\"\n",
    "        return 1 - cosine(\n",
    "            self.document_vectors[doc_id1], self.document_vectors[doc_id2]\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"Quick brown foxes jump over lazy dogs\",\n",
    "        \"The lazy dog sleeps in the sun\",\n",
    "        \"A quick brown dog runs in the park\",\n",
    "        \"Foxes and dogs play in the garden\",\n",
    "    ]\n",
    "\n",
    "    # Initialize and fit model\n",
    "    vsm = VectorSpaceModel(use_idf=True, norm=\"l2\")\n",
    "    vsm.fit(documents)\n",
    "\n",
    "    # Example 1: Search query\n",
    "    print(\"Search Results:\")\n",
    "    query = \"quick brown fox\"\n",
    "    results = vsm.search(query, top_k=3)\n",
    "    for doc_idx, score in results:\n",
    "        print(f\"Document {doc_idx}: '{documents[doc_idx]}' (Score: {score:.3f})\")\n",
    "\n",
    "    # Example 2: Document similarity\n",
    "    print(\"\\nDocument Similarities:\")\n",
    "    for i in range(len(documents) - 1):\n",
    "        sim = vsm.document_similarity(i, i + 1)\n",
    "        print(f\"Similarity between doc {i} and {i+1}: {sim:.3f}\")\n",
    "\n",
    "    # Example 3: Show document vectors\n",
    "    print(\"\\nDocument Vectors (first 5 terms):\")\n",
    "    feature_names = vsm.vectorizer.get_feature_names()[:5]\n",
    "    for i, doc in enumerate(documents):\n",
    "        vector = vsm.document_vectors[i, :5]\n",
    "        print(f\"Doc {i}: {dict(zip(feature_names, vector))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
