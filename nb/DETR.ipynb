{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's understand DETR's architecture by implementing a simplified version. Let's break it down into key components.\n",
    "\n",
    "\n",
    "\n",
    "Let me explain the key components and architecture of DETR:\n",
    "\n",
    "1. Overall Architecture Flow:\n",
    "   - Input image → CNN Backbone → Transformer Encoder → Transformer Decoder → FFNs → Predictions\n",
    "\n",
    "2. Key Components:\n",
    "\n",
    "a) CNN Backbone:\n",
    "```python\n",
    "self.backbone = torchvision.models.resnet50(pretrained=True)\n",
    "```\n",
    "- Uses ResNet50 to extract image features\n",
    "- Converts image to high-level feature map\n",
    "\n",
    "b) Position Embeddings:\n",
    "```python\n",
    "self.pos_embedding = PositionalEncoding(hidden_dim)\n",
    "```\n",
    "- Adds positional information to features\n",
    "- Helps transformer understand spatial relationships\n",
    "\n",
    "c) Transformer:\n",
    "```python\n",
    "self.transformer = nn.Transformer(\n",
    "    d_model=hidden_dim,\n",
    "    nhead=nheads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers\n",
    ")\n",
    "```\n",
    "- Encoder: Processes image features globally\n",
    "- Decoder: Converts object queries into object predictions\n",
    "\n",
    "d) Object Queries:\n",
    "```python\n",
    "self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "```\n",
    "- Learnable embeddings that specify what objects to look for\n",
    "- Fixed number (typically 100) of queries\n",
    "\n",
    "e) Prediction Heads:\n",
    "```python\n",
    "self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
    "self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "```\n",
    "- Class prediction head\n",
    "- Box coordinate prediction head\n",
    "\n",
    "3. Key Innovations:\n",
    "\n",
    "a) No Anchor Boxes:\n",
    "- Uses direct set prediction instead\n",
    "- Learnable object queries replace anchor boxes\n",
    "\n",
    "b) No NMS:\n",
    "- Built-in set prediction eliminates need for post-processing\n",
    "- Each query predicts a unique object\n",
    "\n",
    "c) Global Processing:\n",
    "- Transformer allows global context\n",
    "- All objects processed in parallel\n",
    "\n",
    "Would you like me to:\n",
    "1. Explain the loss function and bipartite matching?\n",
    "2. Show how to train this model?\n",
    "3. Discuss how to use it for inference?\n",
    "4. Dive deeper into any specific component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction logits shape: torch.Size([2, 92])\n",
      "Prediction boxes shape: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, num_layers=6):\n",
    "        super().__init__()\n",
    "        encoder_layer = CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, pos=None):\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, pos=pos)\n",
    "        return self.norm(output)\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, num_layers=6):\n",
    "        super().__init__()\n",
    "        decoder_layer = CustomTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, pos=None, query_pos=None):\n",
    "        output = tgt\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, pos=pos, query_pos=query_pos)\n",
    "        return self.norm(output)\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward(self, src, pos=None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        src2 = self.self_attn(q, k, value=src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward(self, tgt, memory, pos=None, query_pos=None):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        tgt2 = self.multihead_attn(\n",
    "            query=self.with_pos_embed(tgt, query_pos),\n",
    "            key=self.with_pos_embed(memory, pos),\n",
    "            value=memory)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "class SimplifiedDETR(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6, num_queries=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. CNN Backbone (using ResNet50 by default)\n",
    "        self.backbone = torchvision.models.resnet50(pretrained=True)\n",
    "        del self.backbone.fc  # Remove the classification head\n",
    "        \n",
    "        # 2. Position Embeddings\n",
    "        self.pos_embedding = PositionalEncoding(hidden_dim)\n",
    "        \n",
    "        # 3. Input Projection: Convert backbone features to transformer dimensions\n",
    "        self.input_proj = nn.Conv2d(2048, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # 4. Custom Transformer\n",
    "        self.transformer_encoder = CustomTransformerEncoder(\n",
    "            hidden_dim, nheads, dim_feedforward=2048, \n",
    "            dropout=0.1, num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.transformer_decoder = CustomTransformerDecoder(\n",
    "            hidden_dim, nheads, dim_feedforward=2048, \n",
    "            dropout=0.1, num_layers=num_decoder_layers\n",
    "        )\n",
    "        \n",
    "        # 5. Object Queries (learnable parameters)\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        \n",
    "        # 6. Output FFNs (Feed-Forward Networks)\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for no-object class\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)  # 4 for box coordinates\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_queries = num_queries\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Extract features using CNN backbone\n",
    "        features = self.backbone.conv1(x)\n",
    "        features = self.backbone.bn1(features)\n",
    "        features = self.backbone.relu(features)\n",
    "        features = self.backbone.maxpool(features)\n",
    "        \n",
    "        features = self.backbone.layer1(features)\n",
    "        features = self.backbone.layer2(features)\n",
    "        features = self.backbone.layer3(features)\n",
    "        features = self.backbone.layer4(features)\n",
    "        \n",
    "        # 2. Project features to transformer dimension\n",
    "        features_proj = self.input_proj(features)\n",
    "        \n",
    "        # 3. Flatten spatial dimensions and transpose for transformer\n",
    "        batch_size, channels, height, width = features_proj.shape\n",
    "        features_flat = features_proj.flatten(2).permute(2, 0, 1)\n",
    "        \n",
    "        # 4. Generate positional embeddings\n",
    "        pos = self.pos_embedding(features_flat)\n",
    "        \n",
    "        # 5. Object queries\n",
    "        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, batch_size, 1)\n",
    "        tgt = torch.zeros_like(query_embed)\n",
    "        \n",
    "        # 6. Pass through transformer\n",
    "        memory = self.transformer_encoder(features_flat, pos=pos)\n",
    "        hs = self.transformer_decoder(tgt, memory, pos=pos, query_pos=query_embed)\n",
    "        \n",
    "        # 7. Predict classes and boxes\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
    "        \n",
    "        return {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = nn.functional.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # Create model\n",
    "    model = SimplifiedDETR(num_classes=91)  # COCO has 91 classes\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.randn(2, 3, 800, 1200)  # batch_size=2, 3 channels, 800x1200 image\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(x)\n",
    "    \n",
    "    # Print output shapes\n",
    "    print(\"Prediction logits shape:\", outputs['pred_logits'].shape)\n",
    "    print(\"Prediction boxes shape:\", outputs['pred_boxes'].shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
