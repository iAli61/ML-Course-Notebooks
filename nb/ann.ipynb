{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LSH...\n",
      "LSH nearest neighbors to query point: [('point_869', 1.05852149715971), ('point_777', 1.7024391174604139), ('point_251', 1.9113638572205693), ('point_693', 2.1405210115580124), ('point_788', 2.153742934654818)]\n",
      "\n",
      "Testing KD-Tree...\n",
      "KD-Tree nearest neighbors to query point: [('point_113', 2.0106317102460323), ('point_251', 1.9113638572205693), ('point_393', 1.8499312365970935), ('point_777', 1.7024391174604139), ('point_869', 1.05852149715971)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "class LSH:\n",
    "    \"\"\"\n",
    "    Locality Sensitive Hashing implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hash_tables: int, num_hash_functions: int, vector_dimension: int):\n",
    "        self.num_hash_tables = num_hash_tables\n",
    "        self.num_hash_functions = num_hash_functions\n",
    "        self.vector_dimension = vector_dimension\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(num_hash_tables)]\n",
    "        # Random projection vectors for each hash function\n",
    "        self.random_vectors = [\n",
    "            [np.random.randn(vector_dimension) for _ in range(num_hash_functions)]\n",
    "            for _ in range(num_hash_tables)\n",
    "        ]\n",
    "\n",
    "    def _hash_vector(self, vector: np.ndarray, random_vectors: List[np.ndarray]) -> str:\n",
    "        \"\"\"Generate hash for a vector using random projections\"\"\"\n",
    "        hash_bits = []\n",
    "        for rv in random_vectors:\n",
    "            # If projection is positive, hash bit is 1; otherwise 0\n",
    "            hash_bit = 1 if np.dot(vector, rv) >= 0 else 0\n",
    "            hash_bits.append(str(hash_bit))\n",
    "        return ''.join(hash_bits)\n",
    "\n",
    "    def insert(self, vector: np.ndarray, label: str):\n",
    "        \"\"\"Insert a vector into the hash tables\"\"\"\n",
    "        for i, hash_table in enumerate(self.hash_tables):\n",
    "            hash_value = self._hash_vector(vector, self.random_vectors[i])\n",
    "            hash_table[hash_value].append((vector, label))\n",
    "\n",
    "    def query(self, query_vector: np.ndarray, k: int = 1) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find k approximate nearest neighbors\"\"\"\n",
    "        candidates = set()\n",
    "        \n",
    "        # Collect candidates from all hash tables\n",
    "        for i, hash_table in enumerate(self.hash_tables):\n",
    "            hash_value = self._hash_vector(query_vector, self.random_vectors[i])\n",
    "            candidates.update((label, np.linalg.norm(query_vector - vector)) \n",
    "                            for vector, label in hash_table[hash_value])\n",
    "        \n",
    "        # Return k nearest neighbors from candidates\n",
    "        return sorted(candidates, key=lambda x: x[1])[:k]\n",
    "\n",
    "\n",
    "class KDTree:\n",
    "    \"\"\"\n",
    "    KD-Tree implementation for ANN search\n",
    "    \"\"\"\n",
    "    class Node:\n",
    "        def __init__(self, point: np.ndarray, label: str, axis: int):\n",
    "            self.point = point\n",
    "            self.label = label\n",
    "            self.axis = axis\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "\n",
    "    def __init__(self, points: List[np.ndarray], labels: List[str]):\n",
    "        self.dimension = len(points[0])\n",
    "        self.root = self._build_tree(points, labels, 0)\n",
    "\n",
    "    def _build_tree(self, points: List[np.ndarray], labels: List[str], depth: int) -> Node:\n",
    "        if not points:\n",
    "            return None\n",
    "\n",
    "        axis = depth % self.dimension\n",
    "        # Sort points by the current axis\n",
    "        sorted_points_labels = sorted(zip(points, labels), key=lambda x: x[0][axis])\n",
    "        median_idx = len(points) // 2\n",
    "        \n",
    "        node = self.Node(sorted_points_labels[median_idx][0], \n",
    "                        sorted_points_labels[median_idx][1], \n",
    "                        axis)\n",
    "        \n",
    "        node.left = self._build_tree([p for p, _ in sorted_points_labels[:median_idx]],\n",
    "                                   [l for _, l in sorted_points_labels[:median_idx]],\n",
    "                                   depth + 1)\n",
    "        \n",
    "        node.right = self._build_tree([p for p, _ in sorted_points_labels[median_idx + 1:]],\n",
    "                                    [l for _, l in sorted_points_labels[median_idx + 1:]],\n",
    "                                    depth + 1)\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def query(self, query_point: np.ndarray, k: int = 1) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find k approximate nearest neighbors using KD-tree\"\"\"\n",
    "        heap = []  # min heap to store k nearest neighbors\n",
    "        self._search(self.root, query_point, k, heap)\n",
    "        \n",
    "        # Convert heap to sorted list of (label, distance) pairs\n",
    "        return sorted([(label, -dist) for dist, label in heap])\n",
    "\n",
    "    def _search(self, node: Node, query_point: np.ndarray, k: int, heap: List[Tuple[float, str]]):\n",
    "        if not node:\n",
    "            return\n",
    "\n",
    "        distance = np.linalg.norm(query_point - node.point)\n",
    "        \n",
    "        # If heap has less than k elements, add current point\n",
    "        if len(heap) < k:\n",
    "            heapq.heappush(heap, (-distance, node.label))\n",
    "        # If current point is closer than the furthest point in heap\n",
    "        elif -distance > heap[0][0]:\n",
    "            heapq.heapreplace(heap, (-distance, node.label))\n",
    "\n",
    "        axis = node.axis\n",
    "        diff = query_point[axis] - node.point[axis]\n",
    "        \n",
    "        # Recursively search the closer subtree\n",
    "        if diff <= 0:\n",
    "            self._search(node.left, query_point, k, heap)\n",
    "            # Search the other subtree if it might contain closer points\n",
    "            if len(heap) < k or abs(diff) < -heap[0][0]:\n",
    "                self._search(node.right, query_point, k, heap)\n",
    "        else:\n",
    "            self._search(node.right, query_point, k, heap)\n",
    "            if len(heap) < k or abs(diff) < -heap[0][0]:\n",
    "                self._search(node.left, query_point, k, heap)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def demo_ann_algorithms():\n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    num_points = 1000\n",
    "    dimension = 10\n",
    "    \n",
    "    # Generate random points and labels\n",
    "    points = [np.random.randn(dimension) for _ in range(num_points)]\n",
    "    labels = [f\"point_{i}\" for i in range(num_points)]\n",
    "    \n",
    "    # Test LSH\n",
    "    print(\"Testing LSH...\")\n",
    "    lsh = LSH(num_hash_tables=5, num_hash_functions=4, vector_dimension=dimension)\n",
    "    for point, label in zip(points, labels):\n",
    "        lsh.insert(point, label)\n",
    "    \n",
    "    query_point = np.random.randn(dimension)\n",
    "    nearest_neighbors = lsh.query(query_point, k=5)\n",
    "    print(f\"LSH nearest neighbors to query point: {nearest_neighbors}\\n\")\n",
    "    \n",
    "    # Test KD-Tree\n",
    "    print(\"Testing KD-Tree...\")\n",
    "    kdtree = KDTree(points, labels)\n",
    "    nearest_neighbors = kdtree.query(query_point, k=5)\n",
    "    print(f\"KD-Tree nearest neighbors to query point: {nearest_neighbors}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_ann_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ANN benchmarks...\n",
      "Generating 100000 vectors of dimension 128...\n",
      "\n",
      "Testing FAISS with FLAT index:\n",
      "Index building time: 0.02s\n",
      "Search time for 1000 queries: 2.38s\n",
      "Average time per query: 2.38ms\n",
      "\n",
      "Testing FAISS with IVF index:\n",
      "Training time: 0.71s\n",
      "Index building time: 0.21s\n",
      "Search time for 1000 queries: 0.02s\n",
      "Average time per query: 0.02ms\n",
      "\n",
      "Testing FAISS with HNSW index:\n",
      "Index building time: 31.30s\n",
      "Search time for 1000 queries: 0.07s\n",
      "Average time per query: 0.07ms\n",
      "\n",
      "Testing ScaNN:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:27:09.765365: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 10008\n",
      "2024-11-24 16:27:09.799479: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 33.899161ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index building time: 3.72s\n",
      "Search time for 1000 queries: 0.12s\n",
      "Average time per query: 0.12ms\n",
      "\n",
      "Running example usage...\n",
      "\n",
      "Simple Example Usage:\n",
      "\n",
      "FAISS (HNSW) Example:\n",
      "Query results - indices: [7958 6102  890 1396 8362]\n",
      "Distances: [5.577335  5.834095  5.921546  5.9616146 6.050023 ]\n",
      "\n",
      "ScaNN Example:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 16:27:14.058140: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 10000\n",
      "2024-11-24 16:27:14.075021: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 16.790695ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results - indices: [4100 5471 7383 4148 3109]\n",
      "Distances: [20.139353 19.811321 19.656528 19.62415  19.563314]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import scann\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "class FAISSIndexer:\n",
    "    \"\"\"\n",
    "    Wrapper class for FAISS indexing and search\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension: int, index_type: str = 'flat'):\n",
    "        self.dimension = dimension\n",
    "        self.index_type = index_type\n",
    "        self.index = self._create_index()\n",
    "        \n",
    "    def _create_index(self) -> faiss.Index:\n",
    "        if self.index_type == 'flat':\n",
    "            return faiss.IndexFlatL2(self.dimension)\n",
    "        elif self.index_type == 'ivf':\n",
    "            nlist = 100\n",
    "            quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "            return faiss.IndexIVFFlat(quantizer, self.dimension, nlist)\n",
    "        elif self.index_type == 'hnsw':\n",
    "            return faiss.IndexHNSWFlat(self.dimension, 32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {self.index_type}\")\n",
    "\n",
    "    def train(self, vectors: np.ndarray):\n",
    "        if self.index_type == 'ivf':\n",
    "            self.index.train(vectors)\n",
    "    \n",
    "    def add(self, vectors: np.ndarray):\n",
    "        self.index.add(vectors.astype(np.float32))\n",
    "    \n",
    "    def search(self, query: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return self.index.search(query.astype(np.float32), k)\n",
    "\n",
    "class ScaNNIndexer:\n",
    "    \"\"\"\n",
    "    Wrapper class for ScaNN indexing and search with proper batch handling\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dimension: int,\n",
    "                 num_leaves: int = 100,\n",
    "                 num_leaves_to_search: int = 10,\n",
    "                 training_sample_size: int = 10000):\n",
    "        self.dimension = dimension\n",
    "        self.num_leaves = num_leaves\n",
    "        self.num_leaves_to_search = num_leaves_to_search\n",
    "        self.training_sample_size = training_sample_size\n",
    "        self.index = None\n",
    "        \n",
    "    def train(self, vectors: np.ndarray):\n",
    "        \"\"\"Build and train the ScaNN index\"\"\"\n",
    "        self.index = (\n",
    "            scann.scann_ops_pybind.builder(vectors, 10, \"dot_product\")\n",
    "            .tree(\n",
    "                num_leaves=self.num_leaves,\n",
    "                num_leaves_to_search=self.num_leaves_to_search,\n",
    "                training_sample_size=self.training_sample_size,\n",
    "            )\n",
    "            .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "            .reorder(100)\n",
    "            .build()\n",
    "        )\n",
    "    \n",
    "    def search_batch(self, \n",
    "                    queries: np.ndarray, \n",
    "                    k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Search for k nearest neighbors for a batch of queries\n",
    "        Handles queries one by one as ScaNN expects 1D queries\n",
    "        \"\"\"\n",
    "        all_indices = []\n",
    "        all_distances = []\n",
    "        \n",
    "        for query in queries:\n",
    "            indices, distances = self.index.search(query, k)\n",
    "            all_indices.append(indices)\n",
    "            all_distances.append(distances)\n",
    "            \n",
    "        return (np.array(all_indices), np.array(all_distances))\n",
    "\n",
    "def benchmark_ann_methods():\n",
    "    \"\"\"\n",
    "    Benchmark different ANN methods with sample data\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    num_vectors = 100000\n",
    "    dimension = 128\n",
    "    num_queries = 1000\n",
    "    k = 10\n",
    "    \n",
    "    print(f\"Generating {num_vectors} vectors of dimension {dimension}...\")\n",
    "    vectors = np.random.random((num_vectors, dimension)).astype(np.float32)\n",
    "    queries = np.random.random((num_queries, dimension)).astype(np.float32)\n",
    "    \n",
    "    # Test FAISS with different index types\n",
    "    faiss_types = ['flat', 'ivf', 'hnsw']\n",
    "    for index_type in faiss_types:\n",
    "        print(f\"\\nTesting FAISS with {index_type.upper()} index:\")\n",
    "        indexer = FAISSIndexer(dimension, index_type)\n",
    "        \n",
    "        # Train if needed\n",
    "        if index_type in ['ivf']:\n",
    "            start_time = time.time()\n",
    "            indexer.train(vectors)\n",
    "            print(f\"Training time: {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Add vectors\n",
    "        start_time = time.time()\n",
    "        indexer.add(vectors)\n",
    "        print(f\"Index building time: {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Search\n",
    "        start_time = time.time()\n",
    "        distances, indices = indexer.search(queries, k)\n",
    "        search_time = time.time() - start_time\n",
    "        print(f\"Search time for {num_queries} queries: {search_time:.2f}s\")\n",
    "        print(f\"Average time per query: {(search_time/num_queries)*1000:.2f}ms\")\n",
    "        \n",
    "    # Test ScaNN\n",
    "    print(\"\\nTesting ScaNN:\")\n",
    "    indexer = ScaNNIndexer(dimension)\n",
    "    \n",
    "    # Train and build index\n",
    "    start_time = time.time()\n",
    "    indexer.train(vectors)\n",
    "    print(f\"Index building time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    # Search\n",
    "    start_time = time.time()\n",
    "    indices, distances = indexer.search_batch(queries, k)  # Using the new batch search method\n",
    "    search_time = time.time() - start_time\n",
    "    print(f\"Search time for {num_queries} queries: {search_time:.2f}s\")\n",
    "    print(f\"Average time per query: {(search_time/num_queries)*1000:.2f}ms\")\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use FAISS and ScaNN for a simple search task\n",
    "    \"\"\"\n",
    "    # Generate sample data\n",
    "    num_vectors = 10000\n",
    "    dimension = 64\n",
    "    vectors = np.random.random((num_vectors, dimension)).astype(np.float32)\n",
    "    query = np.random.random((1, dimension)).astype(np.float32)\n",
    "    \n",
    "    print(\"\\nSimple Example Usage:\")\n",
    "    \n",
    "    # FAISS example\n",
    "    print(\"\\nFAISS (HNSW) Example:\")\n",
    "    faiss_indexer = FAISSIndexer(dimension, 'hnsw')\n",
    "    faiss_indexer.add(vectors)\n",
    "    distances, indices = faiss_indexer.search(query, k=5)\n",
    "    print(f\"Query results - indices: {indices[0]}\")\n",
    "    print(f\"Distances: {distances[0]}\")\n",
    "    \n",
    "    # ScaNN example\n",
    "    print(\"\\nScaNN Example:\")\n",
    "    scann_indexer = ScaNNIndexer(dimension)\n",
    "    scann_indexer.train(vectors)\n",
    "    # For single query, reshape to ensure it's 2D\n",
    "    indices, distances = scann_indexer.search_batch(query, k=5)\n",
    "    print(f\"Query results - indices: {indices[0]}\")\n",
    "    print(f\"Distances: {distances[0]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running ANN benchmarks...\")\n",
    "    benchmark_ann_methods()\n",
    "    print(\"\\nRunning example usage...\")\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
