{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Flows\n",
    "\n",
    "Think of normalizing flows as a way to transform a simple probability distribution (like a Gaussian) into a more complex one that matches your data. It's similar to how a skilled sculptor can take a plain block of clay and shape it into an intricate statue through a series of careful transformations.\n",
    "\n",
    "The key insight is that we can create complex distributions by applying a sequence of invertible transformations to a simple base distribution. Let's break this down:\n",
    "\n",
    "1) The Base Distribution:\n",
    "We start with a simple distribution that's easy to sample from and evaluate, typically a standard normal distribution. Let's call this z ∼ p(z).\n",
    "\n",
    "2) The Transformation Process:\n",
    "We apply a series of invertible functions (f₁, f₂, ..., fₖ) to transform samples from this simple distribution. Each transformation must be:\n",
    "- Invertible (bijective): Every point in the input space maps to exactly one point in the output space, and vice versa\n",
    "- Differentiable: We need to be able to compute derivatives for training\n",
    "\n",
    "3) The Change of Variables Formula:\n",
    "This is where the \"normalizing\" part comes in. When we transform a probability distribution, we need to account for how the transformation stretches or compresses space. The formula for this is:\n",
    "\n",
    "$$p_x(x) = p_z(z) \\left|\\det\\frac{\\partial f^{-1}(x)}{\\partial x}\\right|$$\n",
    "\n",
    "where x is our transformed variable and z = f⁻¹(x) is the inverse transformation.\n",
    "\n",
    "Here's a concrete example:\n",
    "Imagine you have a dataset of house prices that follows a complex multimodal distribution. You could:\n",
    "1. Start with samples from a normal distribution\n",
    "2. Apply a series of transformations that gradually \"mold\" this distribution into the shape of your house price distribution\n",
    "3. Train the parameters of these transformations by maximizing the likelihood of your data\n",
    "\n",
    "The power of normalizing flows comes from their ability to:\n",
    "- Generate new samples by transforming samples from the base distribution\n",
    "- Compute exact probabilities (unlike VAEs or GANs)\n",
    "- Learn complex distributions through composition of simple transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain the key components of the following implementation for normalization flow:\n",
    "\n",
    "1. Base Distribution and Transformation:\n",
    "We start with a standard normal distribution as our base distribution. The `PlanarFlow` class implements a single transformation layer that modifies the input using the formula:\n",
    "\n",
    "$$f(z) = z + u \\cdot \\tanh(w^T z + b)$$\n",
    "\n",
    "where w, u, and b are learnable parameters. This transformation is invertible under certain conditions and can express complex distributions when composed multiple times.\n",
    "\n",
    "2. Model Architecture:\n",
    "The code includes three main classes:\n",
    "- `PlanarFlow`: Implements a single planar transformation\n",
    "- `NormalizingFlow`: Composes multiple planar transformations\n",
    "- `HousePriceFlow`: Specializes the flow for house price modeling\n",
    "\n",
    "3. Change of Variables Formula:\n",
    "The log probability computation in `HousePriceFlow` implements the change of variables formula:\n",
    "\n",
    "$$\\log p_X(x) = \\log p_Z(z) - \\log \\left|\\det\\frac{\\partial f}{\\partial z}\\right|$$\n",
    "\n",
    "where:\n",
    "- p_Z is the base distribution (standard normal)\n",
    "- z is the transformed variable\n",
    "- The determinant term accounts for the volume change in the transformation\n",
    "\n",
    "4. Training Process:\n",
    "The model is trained by maximizing the log likelihood of the observed data. For house prices, we:\n",
    "- Take the log of prices to make the distribution more manageable\n",
    "- Normalize the log prices to have zero mean and unit variance\n",
    "- Train the flow to learn this normalized distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape check:\n",
      "Batch shape: torch.Size([64, 1])\n",
      "Sample from batch:\n",
      "tensor([[ 0.2014],\n",
      "        [ 1.9023],\n",
      "        [-1.3687],\n",
      "        [-0.3077],\n",
      "        [-1.4579]])\n",
      "Epoch [10/100], Loss: -2.8643\n",
      "Epoch [20/100], Loss: -5.0963\n",
      "Epoch [30/100], Loss: -6.5152\n",
      "Epoch [40/100], Loss: -4.9171\n",
      "Epoch [50/100], Loss: -5.5873\n",
      "Epoch [60/100], Loss: -6.4098\n",
      "Epoch [70/100], Loss: -10.1757\n",
      "Epoch [80/100], Loss: -11.7389\n",
      "Epoch [90/100], Loss: -12.1170\n",
      "Epoch [100/100], Loss: -11.7268\n",
      "\n",
      "Generated house price statistics:\n",
      "Mean: $134787.69\n",
      "Median: $134753.33\n",
      "Std: $536.49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class PlanarFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single layer of Planar Flow transformation.\n",
    "    This is one of the simplest normalizing flow transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # Initialize transformation parameters\n",
    "        self.weight = nn.Parameter(torch.randn(1, dim))      # w vector\n",
    "        self.bias = nn.Parameter(torch.randn(1))             # b scalar\n",
    "        self.scale = nn.Parameter(torch.randn(1, dim))       # u vector\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Compute activation: h(z) = tanh(w·z + b)\n",
    "        activation = torch.tanh(torch.mm(z, self.weight.T) + self.bias)\n",
    "        \n",
    "        # Transform the input: f(z) = z + u·h(z)\n",
    "        z_next = z + self.scale * activation\n",
    "        \n",
    "        # Compute log determinant of Jacobian\n",
    "        phi = (1 - activation**2) * self.weight  # d/dz of tanh\n",
    "        log_det = torch.log(torch.abs(1 + torch.mm(phi, self.scale.T)))\n",
    "        \n",
    "        return z_next, log_det\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete normalizing flow model composed of multiple planar transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_flows):\n",
    "        super().__init__()\n",
    "        # Create a sequence of planar flows\n",
    "        self.flows = nn.ModuleList([PlanarFlow(dim) for _ in range(n_flows)])\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Keep track of total log determinant\n",
    "        total_log_det = torch.zeros(z.size(0), 1).to(z.device)\n",
    "        \n",
    "        # Apply sequence of transformations\n",
    "        for flow in self.flows:\n",
    "            z, log_det = flow(z)\n",
    "            total_log_det += log_det\n",
    "            \n",
    "        return z, total_log_det\n",
    "\n",
    "class HousePriceFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalizing flow model specifically for house prices.\n",
    "    Uses a standard normal as base distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, n_flows=4):\n",
    "        super().__init__()\n",
    "        self.flow = NormalizingFlow(input_dim, n_flows)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform data to base distribution (inverse flow)\n",
    "        z, log_det = self.flow(x)\n",
    "        \n",
    "        # Compute log probability under base distribution (standard normal)\n",
    "        log_prob_base = -0.5 * (z**2 + np.log(2 * np.pi))\n",
    "        \n",
    "        # Compute total log probability using change of variables formula\n",
    "        log_prob = log_prob_base - log_det\n",
    "        \n",
    "        return -torch.mean(log_prob)  # Return negative log likelihood for minimization\n",
    "\n",
    "# Training function\n",
    "def train_flow_model(model, data_loader, n_epochs=100, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            # Extract the tensor from the batch (batch is a tuple from DataLoader)\n",
    "            x = batch[0]  # Get the first element of the batch tuple\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(x)  # Now we pass the tensor, not the batch tuple\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(data_loader):.4f}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic house price data (log-normal distribution)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    log_prices = np.random.normal(12, 0.5, n_samples)  # mu=12 corresponds to ~$160k median price\n",
    "    prices = np.exp(log_prices)\n",
    "    \n",
    "    # Normalize data\n",
    "    prices_normalized = (np.log(prices) - np.mean(log_prices)) / np.std(log_prices)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    prices_tensor = torch.FloatTensor(prices_normalized).reshape(-1, 1)\n",
    "    dataset = torch.utils.data.TensorDataset(prices_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = HousePriceFlow(input_dim=1, n_flows=4)\n",
    "    \n",
    "    # Debug information\n",
    "    print(\"Data shape check:\")\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        print(f\"Batch shape: {x.shape}\")\n",
    "        print(f\"Sample from batch:\\n{x[:5]}\")  # Print first 5 values\n",
    "        break\n",
    "    \n",
    "    train_flow_model(model, dataloader)\n",
    "    \n",
    "    # Generate new samples\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1000, 1)  # Sample from base distribution\n",
    "        samples, _ = model.flow(z)\n",
    "        \n",
    "        # Transform back to original scale\n",
    "        samples = samples.numpy()\n",
    "        samples = np.exp(samples * np.std(log_prices) + np.mean(log_prices))\n",
    "        \n",
    "    print(\"\\nGenerated house price statistics:\")\n",
    "    print(f\"Mean: ${samples.mean():.2f}\")\n",
    "    print(f\"Median: ${np.median(samples):.2f}\")\n",
    "    print(f\"Std: ${samples.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used with intermediates: 6 tensors\n",
      "Memory used without intermediates: 1 tensor\n",
      "\n",
      "Reconstruction error:\n",
      "Max difference: 0.798676\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InvertibleLayer(nn.Module):\n",
    "    \"\"\"A simple invertible layer for demonstration\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(dim, dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + torch.tanh(x @ self.weight)\n",
    "        \n",
    "    def inverse(self, y):\n",
    "        # In practice, you might need to use fixed-point iteration\n",
    "        # This is a simplified inverse for demonstration\n",
    "        x = y\n",
    "        for _ in range(5):  # Fixed-point iteration\n",
    "            x = y - torch.tanh(x @ self.weight)\n",
    "        return x\n",
    "\n",
    "class MemoryEfficientNetwork(nn.Module):\n",
    "    def __init__(self, dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([InvertibleLayer(dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Standard forward pass - store all intermediates\n",
    "        intermediates = [x]\n",
    "        current = x\n",
    "        for layer in self.layers:\n",
    "            current = layer(current)\n",
    "            intermediates.append(current)\n",
    "        return current, intermediates\n",
    "    \n",
    "    def forward_memory_efficient(self, x):\n",
    "        # Memory efficient forward pass - only store final output\n",
    "        current = x\n",
    "        for layer in self.layers:\n",
    "            current = layer(current)\n",
    "        return current\n",
    "        \n",
    "    def reconstruct_intermediate(self, output, layer_index):\n",
    "        # Reconstruct a specific intermediate activation\n",
    "        current = output\n",
    "        for layer in reversed(self.layers[layer_index:]):\n",
    "            current = layer.inverse(current)\n",
    "        return current\n",
    "\n",
    "# Example usage\n",
    "dim = 10\n",
    "num_layers = 5\n",
    "model = MemoryEfficientNetwork(dim, num_layers)\n",
    "x = torch.randn(1, dim)\n",
    "\n",
    "# Standard forward pass (storing all intermediates)\n",
    "output_standard, intermediates = model(x)\n",
    "print(f\"Memory used with intermediates: {len(intermediates)} tensors\")\n",
    "\n",
    "# Memory efficient forward pass\n",
    "output_efficient = model.forward_memory_efficient(x)\n",
    "print(f\"Memory used without intermediates: 1 tensor\")\n",
    "\n",
    "# Reconstruct middle layer activation\n",
    "reconstructed = model.reconstruct_intermediate(output_efficient, layer_index=2)\n",
    "print(\"\\nReconstruction error:\")\n",
    "print(f\"Max difference: {(intermediates[2] - reconstructed).abs().max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
