{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize model\\nssl_model = TeacherStudentSSL(feature_dim=256, ema_decay=0.99)\\n\\n# Train\\ntrain_ssl(ssl_model, train_loader, epochs=100)\\n\\n# After training, use teacher for downstream tasks\\ntrained_encoder = ssl_model.teacher\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import AdamW\n",
    "from copy import deepcopy\n",
    "\n",
    "class EncoderNetwork(nn.Module):\n",
    "    \"\"\"Base encoder network to generate representations.\"\"\"\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        # Simple ConvNet backbone\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        \n",
    "        # Projection MLP\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, feature_dim),\n",
    "        )\n",
    "        \n",
    "        # Prediction MLP (only used in student)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, feature_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get backbone features\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Project features\n",
    "        z = self.projector(x)\n",
    "        z = F.normalize(z, dim=-1)  # L2 normalize\n",
    "        \n",
    "        # Predict (for student)\n",
    "        p = self.predictor(z)\n",
    "        p = F.normalize(p, dim=-1)  # L2 normalize\n",
    "        \n",
    "        return z, p\n",
    "\n",
    "class TeacherStudentSSL:\n",
    "    \"\"\"Teacher-Student framework for self-supervised learning.\"\"\"\n",
    "    def __init__(self, feature_dim=256, ema_decay=0.99):\n",
    "        self.student = EncoderNetwork(feature_dim)\n",
    "        # Teacher is a moving average of the student\n",
    "        self.teacher = deepcopy(self.student)\n",
    "        self.teacher.predictor = None  # Teacher doesn't need predictor\n",
    "        \n",
    "        # Disable gradients for teacher\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.ema_decay = ema_decay\n",
    "        \n",
    "        # Define augmentations\n",
    "        self.augment = T.Compose([\n",
    "            T.RandomResizedCrop(32),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            T.GaussianBlur(kernel_size=3),\n",
    "        ])\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self):\n",
    "        \"\"\"Update teacher weights as EMA of student weights.\"\"\"\n",
    "        for teacher_param, student_param in zip(self.teacher.parameters(), \n",
    "                                              self.student.parameters()):\n",
    "            teacher_param.data = (self.ema_decay * teacher_param.data + \n",
    "                                (1 - self.ema_decay) * student_param.data)\n",
    "    \n",
    "    def training_step(self, images):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        # Generate two random augmentations\n",
    "        view1 = torch.stack([self.augment(img) for img in images])\n",
    "        view2 = torch.stack([self.augment(img) for img in images])\n",
    "        \n",
    "        # Student forward passes\n",
    "        student_z1, student_p1 = self.student(view1)\n",
    "        student_z2, student_p2 = self.student(view2)\n",
    "        \n",
    "        # Teacher forward passes (no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_z1, _ = self.teacher(view1)\n",
    "            teacher_z2, _ = self.teacher(view2)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss1 = F.mse_loss(student_p1, teacher_z2.detach())\n",
    "        loss2 = F.mse_loss(student_p2, teacher_z1.detach())\n",
    "        loss = (loss1 + loss2) * 0.5\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def train_ssl(model, train_loader, epochs=100, lr=1e-3):\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "    optimizer = AdamW(model.student.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            # Training step\n",
    "            loss = model.training_step(images)\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update teacher\n",
    "            model.update_teacher()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Print epoch stats\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize model\n",
    "ssl_model = TeacherStudentSSL(feature_dim=256, ema_decay=0.99)\n",
    "\n",
    "# Train\n",
    "train_ssl(ssl_model, train_loader, epochs=100)\n",
    "\n",
    "# After training, use teacher for downstream tasks\n",
    "trained_encoder = ssl_model.teacher\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
